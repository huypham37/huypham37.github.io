<!DOCTYPE html>
<html>

<head>
	<meta name="generator" content="Hugo 0.144.2"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <title>Your Name - Website</title>
    <link rel="stylesheet" href="/css/style.css">
    <!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/css/style.css">
    <title>Your Name - Website</title>
    <meta name="description" content="">
</head>
</html>
</head>

<body>
    <!DOCTYPE html>
<html>

<head>
    <title>Your Name - Website</title>
    <link rel="stylesheet" href="/css/style.css">
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
    <header>
        
        
        <nav>
            <ul>
                <li><a href="/about/">About Me</a></li>
                <li><a href="/mywork/">My Work</a></li>
                <li><a href="/til/">TIL</a></li>
                <li><a href="/tutorials/">Tutorials</a></li>
                <li><a href="https://github.com/yourusername" target="_blank">GitHub</a></li>
                <li><a href="https://linkedin.com/in/yourusername" target="_blank">LinkedIn</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <div class="sections-container">
            <section id="til">
                <h2>Today I Learned (TIL)</h2>
                <div class="til-articles">
                    
                    
                    <article class="til-article">
                        <h3>good quote for today</h3>
                        <p><small>April 3, 2025</small></p>
                        <div class="til-tags">
                            <strong>Tags:</strong>
                            
                            <a href="http://localhost:1313/tags/quote" class="tag">quote</a>
                            
                            <a href="http://localhost:1313/tags/ai" class="tag">AI</a>
                            
                            <a href="http://localhost:1313/tags/ai-in-code" class="tag">ai-in-code</a>
                            
                        </div>
                        <div class="til-content">
                            <p>I went through this nice quote that truly align with what I thought for a very long time.</p>
<p>&ldquo;If you&rsquo;re a new programmer, my optimistic version is that there has never been a better time to learn to program, because it shaves down the learning curve so much. When you&rsquo;re learning to program and you miss a semicolon and you bang your head against the computer for four hours [&hellip;] if you&rsquo;re unlucky you quit programming for good because it was so frustrating. [&hellip;]</p>
<p>I&rsquo;ve always been a project-oriented learner; I can learn things by building something, and now the friction involved in building something has gone down so much [&hellip;] So I think especially if you&rsquo;re an autodidact, if you&rsquo;re somebody who likes teaching yourself things, these are a gift from heaven. You get a weird teaching assistant that knows loads of stuff and occasionally makes weird mistakes and believes in bizarre conspiracy theories, but you have 24 hour access to that assistant.</p>
<p>If you&rsquo;re somebody who prefers structured learning in classrooms, I think the benefits are going to take a lot longer to get to you because we don&rsquo;t know how to use these things in classrooms yet. [&hellip;]</p>
<p>If you want to strike out on your own, this is an amazing tool if you learn how to learn with it. So you&rsquo;ve got to learn the limits of what it can do, and you&rsquo;ve got to be disciplined enough to make sure you&rsquo;re not outsourcing the bits you need to learn to the machines.&rdquo; - Simon Willison.</p>
<p>Well, more people should code now, because it is never a better time.</p>
<p><a href="/til/08_morepeopleshouldcode/">More People Should Code</a></p>
<p><a href="/til/07-whyIloveLLM/">Why I Love Working with LLMs</a></p>

                        </div>
                    </article>
                    <hr>
                    
                    <article class="til-article">
                        <h3>Interesting Anthropic Paper</h3>
                        <p><small>April 3, 2025</small></p>
                        <div class="til-tags">
                            <strong>Tags:</strong>
                            
                            <a href="http://localhost:1313/tags/research-papers" class="tag">research-papers</a>
                            
                            <a href="http://localhost:1313/tags/ai" class="tag">AI</a>
                            
                            <a href="http://localhost:1313/tags/ai-in-code" class="tag">ai-in-code</a>
                            
                        </div>
                        <div class="til-content">
                            <p>I went through these two papers which I found truly interesting for anyone has affinity in the development of LLM.</p>
<p><a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html">Circuit Tracing: Revealing Computational Graphs in Language Models</a>.</p>
<p><a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">On the Biology of a Large Language Model</a></p>

                        </div>
                    </article>
                    <hr>
                    
                    <article class="til-article">
                        <h3>Why I love writing</h3>
                        <p><small>April 3, 2025</small></p>
                        <div class="til-tags">
                            <strong>Tags:</strong>
                            
                            <a href="http://localhost:1313/tags/writing" class="tag">writing</a>
                            
                            <a href="http://localhost:1313/tags/ai" class="tag">ai</a>
                            
                            <a href="http://localhost:1313/tags/ai-in-code" class="tag">ai-in-code</a>
                            
                            <a href="http://localhost:1313/tags/ai-in-content" class="tag">ai-in-content</a>
                            
                        </div>
                        <div class="til-content">
                            <p>&ldquo;AI will write 90% of the code for software engineers within the next three to six months and every line of code within the next year.&quot;-Anthropic CEO Dario Amodei</p>
<p>I would think the same with bullsh** internet contents in the three to six months.</p>
<p>That is why I love writing.</p>
<p>Even my writing sucks, grammars, vocabularies&hellip;.</p>
<p>But who cares, when I dies, this writing is actuall from me, with horrible grammar errors, with terrible vocabs that can bore anyone from the first word. But it is mine,
not from the freaking robot (AGI, huh?).</p>
<p>I believe, my content, and my idea, my thought or anyone&rsquo;s manual writing (sh**, now I have to use &ldquo;manual&rdquo; for writing), thought would be a treasure in this digital realm.</p>
<p>That&rsquo;s why I write, and will write.</p>

                        </div>
                    </article>
                    <hr>
                    
                    <article class="til-article">
                        <h3>More people should code</h3>
                        <p><small>March 19, 2025</small></p>
                        <div class="til-tags">
                            <strong>Tags:</strong>
                            
                            <a href="http://localhost:1313/tags/ai" class="tag">AI</a>
                            
                            <a href="http://localhost:1313/tags/writing" class="tag">writing</a>
                            
                            <a href="http://localhost:1313/tags/coding" class="tag">coding</a>
                            
                        </div>
                        <div class="til-content">
                            <p>I came across this post this morning, and Andrew Ng’s thoughts about the future of AI resonated with my own thoughts about the future of coding.</p>
<p>“As coding becomes easier, more people should code, not fewer!”</p>
<ul>
<li><a href="https://www.deeplearning.ai/the-batch/issue-292/">Andrew Ng</a></li>
</ul>
<p>Let’s consider the launch of GPT-3.5 by OpenAI in 2020. Could we ask it to brainstorm 32 ideas that will make AI useful in the next five years? We couldn’t, and even now, we can’t.</p>
<p>In 2025, five years after GPT-3.5, we have more useful tools built from AI, such as DeepSearch and AI-Agent. These tools are the result of human hands and brains, not AI ideas.</p>
<p>AI excels at repetitive tasks like searching, synthesizing information, and more searching. However, it lacks the ability to come up with new ideas that will potentially change the world.</p>

                        </div>
                    </article>
                    <hr>
                    
                    <article class="til-article">
                        <h3>Why I Love Working with Large Language Models</h3>
                        <p><small>March 14, 2025</small></p>
                        <div class="til-tags">
                            <strong>Tags:</strong>
                            
                            <a href="http://localhost:1313/tags/ai" class="tag">ai</a>
                            
                            <a href="http://localhost:1313/tags/llm" class="tag">llm</a>
                            
                            <a href="http://localhost:1313/tags/productivity" class="tag">productivity</a>
                            
                            <a href="http://localhost:1313/tags/programming" class="tag">programming</a>
                            
                        </div>
                        <div class="til-content">
                            <p>Because they are stupid.</p>
<p>Normal people workflows with LLMs are like this, and think they can learn from them:</p>
<ul>
<li>I have a problem X</li>
<li>I ask LLM to solve it</li>
<li>LLM gives me a solution</li>
<li>I take the solution and run it</li>
<li>It works, I am happy</li>
<li>Then I ask LLM to explain the solution -&gt; I learned it.</li>
</ul>
<p>It&rsquo;s fine, but in the best case.</p>
<p>Then how I work with LLMs:</p>
<ul>
<li>I have a problem X</li>
<li>I ask LLM to solve it</li>
<li>LLM gives me a solution</li>
<li>This goddamn solution is hallucinated, the apis was from the sky (but sometimes I found they come up better name than the code owners)</li>
<li>I go read the documents, read the code of the LLMs.</li>
<li>Fix it, it works, I am happy.</li>
</ul>
<p>Instead of passively ask LLM to explain the solution, I actually just solve the problem by myself just by stepping backward and looking at the problem.</p>
<p>I think it&rsquo;s better.</p>

                        </div>
                    </article>
                    <hr>
                    
                    <article class="til-article">
                        <h3>XOR Operations: Clever Applications Beyond Basic Logic</h3>
                        <p><small>March 13, 2025</small></p>
                        <div class="til-tags">
                            <strong>Tags:</strong>
                            
                            <a href="http://localhost:1313/tags/programming" class="tag">programming</a>
                            
                            <a href="http://localhost:1313/tags/algorithms" class="tag">algorithms</a>
                            
                            <a href="http://localhost:1313/tags/bit-manipulation" class="tag">bit-manipulation</a>
                            
                            <a href="http://localhost:1313/tags/xor" class="tag">xor</a>
                            
                        </div>
                        <div class="til-content">
                            <p>XOR (exclusive OR) is a fundamental bitwise operation that returns true only when inputs differ. While it seems simple, XOR has powerful applications beyond basic logic gates that every programmer should know.</p>
<ul>
<li><strong>Toggling Bits:</strong> XOR provides an elegant way to flip specific bits without affecting others.</li>
</ul>
<p>Let x = 9 (binary 1001) Toggle the 3rd bit with x ^ 0x04 (0x04 = 0100) 1001 ^ 0100 = 1101 = 13 (3rd bit flips from 0 to 1)</p>
<img src="/img/toggle-bits.png" alt="Toggling Bits with XOR" width="700" height="400">
<ul>
<li><strong>Swapping Values:</strong> XOR enables swapping two variables without using a temporary variable.</li>
</ul>
<p>Let a = 5 (0101), b = 3 (0011) a ^= b: 0101 ^ 0011 = 0110 (a = 6) b ^= a: 0011 ^ 0110 = 0101 (b = 5) a ^= b: 0110 ^ 0101 = 0011 (a = 3) Result: a = 3, b = 5 (swapped!)</p>
<img src="/img/swap-xor.png" alt="Swapping Values with XOR" width="700" height="400">
<ul>
<li><strong>Finding Unique Number:</strong> XOR can find a single non-repeated element in an array where all other elements appear exactly twice.
Array: [3, 5, 3, 4, 4] XOR all: 3 ^ 5 ^ 3 ^ 4 ^ 4 3 ^ 3 = 0 (pairs cancel), 4 ^ 4 = 0, leaving 5 Result: 5 (the number appearing once)</li>
</ul>
<p>The magic of XOR lies in its key properties: it&rsquo;s commutative (order doesn&rsquo;t matter), associative (grouping doesn&rsquo;t matter), self-inverse (a number XORed with itself gives 0), and identity element (any number XORed with 0 is itself). These properties make XOR particularly useful in cryptography, memory-efficient algorithms, and embedded systems programming.</p>

                        </div>
                    </article>
                    <hr>
                    
                    <article class="til-article">
                        <h3>Everyone talks about Agentic AI</h3>
                        <p><small>February 25, 2025</small></p>
                        <div class="til-tags">
                            <strong>Tags:</strong>
                            
                            <a href="http://localhost:1313/tags/agentic-ai" class="tag">agentic-ai</a>
                            
                            <a href="http://localhost:1313/tags/insights" class="tag">insights</a>
                            
                            <a href="http://localhost:1313/tags/tips" class="tag">tips</a>
                            
                        </div>
                        <div class="til-content">
                            <p>“Big data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it.” - Dan Ariely
This is still true with current agentic ai discussion.</p>

                        </div>
                    </article>
                    <hr>
                    
                    <article class="til-article">
                        <h3>Non-Agentic vs. Agentic AI: Beyond the Generative Hype</h3>
                        <p><small>February 24, 2025</small></p>
                        <div class="til-tags">
                            <strong>Tags:</strong>
                            
                            <a href="http://localhost:1313/tags/ai" class="tag">ai</a>
                            
                            <a href="http://localhost:1313/tags/generative-ai" class="tag">generative-ai</a>
                            
                            <a href="http://localhost:1313/tags/agentic-ai" class="tag">agentic-ai</a>
                            
                        </div>
                        <div class="til-content">
                            <p>The hype of generative AI is gone; people now are more familiar with what a generative AI can do. But I think there is a lot more an AI (or specifically a LLM) can do, more than just &ldquo;Write me an essay in 4000 words like a first year college student.&rdquo;</p>
<ul>
<li><strong>Non-agentic AI:</strong> This is basically a generative AI which you give it an input, give you an output, no recheck, no question, no feedback. It&rsquo;s like a robot that you give it a command, it does it, and that&rsquo;s it.</li>
</ul>
<img src="/img/non-agnetic-flow.png" alt="Non-Agentic AI Workflow" width="700" height="400">
<ul>
<li><strong>Agentic AI:</strong> A generative AI with a set of tools. The newest application of agentic AI is deep research or deep search from multiple frontier AI companies like OpenAI, Deepseek or Google - AI which can talk to an API, send requests to websites, surf the web, re-check the value and produce a result. The typical agentic AI workflow would look like this:</li>
</ul>
<img src="/img/agentic-ai-flow.png" alt="Agentic AI Workflow" width="700" height="400">

                        </div>
                    </article>
                    <hr>
                    
                    <article class="til-article">
                        <h3>What I&#39;ve Learned from Pair Programming with Copilot</h3>
                        <p><small>February 23, 2025</small></p>
                        <div class="til-tags">
                            <strong>Tags:</strong>
                            
                            <a href="http://localhost:1313/tags/copilot" class="tag">copilot</a>
                            
                            <a href="http://localhost:1313/tags/pair-programming" class="tag">pair-programming</a>
                            
                            <a href="http://localhost:1313/tags/tips" class="tag">tips</a>
                            
                        </div>
                        <div class="til-content">
                            <ul>
<li>
<p>Always generate a comprehensive test suite and integration test suite after writing the code (for each module and for modules working together).</p>
</li>
<li>
<p>Copilot is not a magical tool; it can&rsquo;t create wonders on its own. It&rsquo;s best to ask it to assist with repetitive tasks or generate boilerplate code.</p>
</li>
<li>
<p>Don&rsquo;t fall into the trap of relying solely on Copilot. Read the error messages from the terminal or test output. I used to spend two hours repeatedly passing terminal failed error messages to Copilot, asking it to fix them, but it couldn&rsquo;t. The loop would continue until I read the error message and fixed it myself.</p>
</li>
<li>
<p>However, Copilot can be helpful in a way. It can prevent you from writing overly complex code by suggesting that &ldquo;Just do it.&rdquo; By removing the initial barrier of syntax and structure, you can start writing code without overthinking it.</p>
</li>
</ul>

                        </div>
                    </article>
                    <hr>
                    
                    <article class="til-article">
                        <h3>Pipelining vs Concurrency: Understanding the Difference</h3>
                        <p><small>February 23, 2025</small></p>
                        <div class="til-tags">
                            <strong>Tags:</strong>
                            
                            <a href="http://localhost:1313/tags/pipelining" class="tag">pipelining</a>
                            
                            <a href="http://localhost:1313/tags/concurrency" class="tag">concurrency</a>
                            
                            <a href="http://localhost:1313/tags/hardware" class="tag">hardware</a>
                            
                            <a href="http://localhost:1313/tags/software" class="tag">software</a>
                            
                        </div>
                        <div class="til-content">
                            <p>Today I learned about the key differences between pipelining and concurrency:</p>
<h2 id="pipelining">Pipelining</h2>
<ul>
<li><strong>Low-level Hardware Technique:</strong> Breaks down instruction execution into stages like fetch, decode, execute, memory access, and write-back.</li>
<li><strong>Assembly Line Approach:</strong> Multiple instructions are processed simultaneously with each in a different stage, enabling a new instruction per clock cycle.</li>
<li><strong>Improved Throughput:</strong> Enhances processing without the need to increase the clock speed.</li>
</ul>
<h2 id="concurrency">Concurrency</h2>
<ul>
<li><strong>Higher-level Concept:</strong> Deals with multiple computations happening during overlapping time periods.</li>
<li><strong>Various Models:</strong> Implemented via multithreading, multiprocessing, or asynchronous programming.</li>
<li><strong>Design Focus:</strong> Emphasizes logical task parallelism and efficient program design rather than instruction-level execution.</li>
</ul>
<h2 id="key-distinction">Key Distinction</h2>
<ul>
<li><strong>Pipelining:</strong> Implemented in the CPU hardware, focusing on instruction-level parallelism.</li>
<li><strong>Concurrency:</strong> Managed by software and operating systems to run multiple tasks simultaneously.</li>
</ul>
<p>Understanding these concepts helps align hardware execution with software design for more efficient systems.</p>
<p>Happy Learning!</p>

                        </div>
                    </article>
                    <hr>
                    
                    <article class="til-article">
                        <h3>Computing π with OpenMP</h3>
                        <p><small>February 23, 2025</small></p>
                        <div class="til-tags">
                            <strong>Tags:</strong>
                            
                            <a href="http://localhost:1313/tags/openmp" class="tag">openmp</a>
                            
                            <a href="http://localhost:1313/tags/parallel-computing" class="tag">parallel-computing</a>
                            
                            <a href="http://localhost:1313/tags/cpp" class="tag">cpp</a>
                            
                        </div>
                        <div class="til-content">
                            <h2 id="overview">Overview</h2>
<ul>
<li>
<p><strong>Numerical Integration for π:</strong><br>
The function computes π by integrating the function <code>4/(1+x^2)</code> over [0,1] using a Riemann sum with midpoints.</p>
</li>
<li>
<p><strong>Timing with std::chrono:</strong><br>
The duration for the computation is measured using C++11&rsquo;s <code>&lt;chrono&gt;</code> utilities.</p>
</li>
</ul>
<h2 id="using-openmp">Using OpenMP</h2>
<ul>
<li>
<p><strong>Parallel Region:</strong><br>
The computation is performed inside an OpenMP parallel region where each thread calculates a partial sum.</p>
</li>
<li>
<p><strong>Local Sum per Thread:</strong><br>
Each thread uses its own <code>local_sum</code> variable to accumulate values from its portion of the loop. This prevents race conditions.</p>
</li>
<li>
<p><strong>Combining Results:</strong><br>
There are two common ways to combine the thread-local results:</p>
<ol>
<li><strong>Critical Section:</strong>
<ul>
<li>Wrap the update of <code>sum</code> in a <code>#pragma omp critical</code> block.</li>
<li>This ensures only one thread updates <code>sum</code> at a time.</li>
<li><strong>Downside:</strong> Overhead due to thread contention.</li>
</ul>
</li>
<li><strong>Reduction Clause:</strong>
<ul>
<li>Use <code>#pragma omp for reduction(+:sum)</code> which provides each thread with a private copy of <code>sum</code>.</li>
<li>OpenMP automatically handles the combination after the loop.</li>
<li><strong>Advantage:</strong> More efficient with less overhead compared to a critical section.</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="what-we-observed">What We Observed</h2>
<ul>
<li>
<p><strong>Performance:</strong><br>
Removing the critical section can show faster performance since threads don&rsquo;t have to wait. However, without any mechanism (like reduction) to combine the results, the final value of <code>sum</code> would be computed incorrectly.</p>
</li>
<li>
<p><strong>Correctness vs. Overhead:</strong><br>
The critical directive ensures correctness by serializing the merging of results, but at the cost of performance. The reduction clause offers both correctness and better performance.</p>
</li>
</ul>
<h2 id="key-takeaways">Key Takeaways</h2>
<ul>
<li>
<p>The critical directive allows only one thread to execute a block at a time, ensuring safe updates to shared variables but can slow down the program.</p>
</li>
<li>
<p>The reduction clause creates private copies of a variable for each thread, automatically merging them after execution, which minimizes synchronization overhead.</p>
</li>
</ul>
<p>This summary encapsulates the key insights related to parallel computation and synchronization using OpenMP as discussed in the context of the <code>compute_pi.cpp</code> file.</p>

                        </div>
                    </article>
                    <hr>
                    
                    <article class="til-article">
                        <h3>std::cout is Not Thread-Safe: Using stringstream for Safe Multithreaded Output in C&#43;&#43;</h3>
                        <p><small>February 23, 2025</small></p>
                        <div class="til-tags">
                            <strong>Tags:</strong>
                            
                            <a href="http://localhost:1313/tags/cpp" class="tag">cpp</a>
                            
                            <a href="http://localhost:1313/tags/multithreading" class="tag">multithreading</a>
                            
                        </div>
                        <div class="til-content">
                            <p>Today I learned that <code>std::cout</code> is not thread-safe by default. When multiple threads try to write to <code>std::cout</code> simultaneously, the output can become jumbled and garbled due to race conditions.</p>
<h2 id="the-problem">The Problem</h2>
<p>Consider this problematic code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">#pragma omp parallel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>{
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> thread_id <span style="color:#f92672">=</span> omp_get_num_threads();
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>cout <span style="color:#f92672">&lt;&lt;</span> <span style="color:#e6db74">&#34;Hello from thread &#34;</span> <span style="color:#f92672">&lt;&lt;</span> thread_id <span style="color:#f92672">&lt;&lt;</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>This can produce jumbled output like:</p>
<pre tabindex="0"><code>Hello Hello from thread from thread 12
Hello from thread 3
</code></pre><h2 id="the-solution-using-stringstream">The Solution: Using stringstream</h2>
<p>A better approach is to use <code>std::stringstream</code> to build the complete string in thread-local storage before writing to <code>std::cout</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">#pragma omp parallel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>{
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> thread_id <span style="color:#f92672">=</span> omp_get_num_threads();
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>stringstream ss;
</span></span><span style="display:flex;"><span>    ss <span style="color:#f92672">&lt;&lt;</span> <span style="color:#e6db74">&#34;Hello from thread &#34;</span> <span style="color:#f92672">&lt;&lt;</span> thread_id <span style="color:#f92672">&lt;&lt;</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>;
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// Convert to string first
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    std<span style="color:#f92672">::</span>string output <span style="color:#f92672">=</span> ss.str();
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// Write the complete string at once
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    std<span style="color:#f92672">::</span>cout <span style="color:#f92672">&lt;&lt;</span> output;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="why-this-works">Why This Works</h2>
<ol>
<li><code>std::stringstream</code> creates a thread-local buffer for building the output string</li>
<li>The string construction happens entirely within each thread</li>
<li>The final write to <code>std::cout</code> is more likely to be atomic when writing a single string</li>
<li>Each thread maintains its own independent stringstream, preventing race conditions</li>
</ol>
<h2 id="alternative-using-critical-sections">Alternative: Using Critical Sections</h2>
<p>If you really need to use <code>std::cout</code> directly, you can also use OpenMP&rsquo;s critical section:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">#pragma omp parallel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>{
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> thread_id <span style="color:#f92672">=</span> omp_get_num_threads();
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#pragma omp critical
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    {
</span></span><span style="display:flex;"><span>        std<span style="color:#f92672">::</span>cout <span style="color:#f92672">&lt;&lt;</span> <span style="color:#e6db74">&#34;Hello from thread &#34;</span> <span style="color:#f92672">&lt;&lt;</span> thread_id <span style="color:#f92672">&lt;&lt;</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>However, using <code>stringstream</code> is often more efficient as it minimizes the time spent in critical sections.</p>
<h2 id="key-takeaway">Key Takeaway</h2>
<p>When dealing with multithreaded output in C++, always remember that <code>std::cout</code> operations are not thread-safe. Use <code>std::stringstream</code> to build output strings locally before writing them, or properly synchronize access to <code>std::cout</code> using critical sections or mutexes.</p>

                        </div>
                    </article>
                    <hr>
                    
                    <article class="til-article">
                        <h3>Setting Up OpenMP on macOS</h3>
                        <p><small>February 22, 2025</small></p>
                        <div class="til-tags">
                            <strong>Tags:</strong>
                            
                            <a href="http://localhost:1313/tags/openmp" class="tag">openmp</a>
                            
                            <a href="http://localhost:1313/tags/macos" class="tag">macos</a>
                            
                            <a href="http://localhost:1313/tags/clang" class="tag">clang</a>
                            
                            <a href="http://localhost:1313/tags/llvm" class="tag">llvm</a>
                            
                            <a href="http://localhost:1313/tags/cmake" class="tag">cmake</a>
                            
                        </div>
                        <div class="til-content">
                            <h2 id="problem-description">Problem Description</h2>
<p>When trying to compile OpenMP code on macOS, you might encounter this error:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>fatal error: <span style="color:#e6db74">&#39;omp.h&#39;</span> file not found
</span></span><span style="display:flex;"><span><span style="color:#75715e">#include &lt;omp.h&gt;</span>
</span></span><span style="display:flex;"><span>         ^~~~~~~
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span> error generated.
</span></span></code></pre></div><p>This occurs because Apple&rsquo;s default Clang compiler doesn&rsquo;t include OpenMP support out of the box.</p>
<h2 id="solution">Solution</h2>
<h3 id="1-install-llvm-using-homebrew">1. Install LLVM using Homebrew</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>brew install llvm
</span></span></code></pre></div><h3 id="2-set-up-environment-variables">2. Set up Environment Variables</h3>
<p>Add these lines to your <code>~/.zshrc</code> (or <code>~/.bash_profile</code>):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># LLVM/OpenMP configuration</span>
</span></span><span style="display:flex;"><span>export PATH<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/opt/homebrew/opt/llvm/bin:</span>$PATH<span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>export CC<span style="color:#f92672">=</span>/opt/homebrew/opt/llvm/bin/clang
</span></span><span style="display:flex;"><span>export CXX<span style="color:#f92672">=</span>/opt/homebrew/opt/llvm/bin/clang++
</span></span><span style="display:flex;"><span>export LDFLAGS<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;-L/opt/homebrew/opt/llvm/lib&#34;</span>
</span></span><span style="display:flex;"><span>export CPPFLAGS<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;-I/opt/homebrew/opt/llvm/include&#34;</span>
</span></span></code></pre></div><p>Then reload your shell configuration:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>source ~/.zshrc
</span></span></code></pre></div><h3 id="3-update-cmakeliststxt">3. Update CMakeLists.txt</h3>
<p>Force CMake to use LLVM&rsquo;s Clang before the project declaration:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cmake" data-lang="cmake"><span style="display:flex;"><span>cmake_minimum_required(<span style="color:#e6db74">VERSION</span> <span style="color:#e6db74">3.14</span>)<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># Force CMake to use LLVM compiler before project() declaration
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>set(<span style="color:#e6db74">CMAKE_C_COMPILER</span> <span style="color:#e6db74">&#34;/opt/homebrew/opt/llvm/bin/clang&#34;</span>)<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>set(<span style="color:#e6db74">CMAKE_CXX_COMPILER</span> <span style="color:#e6db74">&#34;/opt/homebrew/opt/llvm/bin/clang++&#34;</span>)<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span>project(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">openmp</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">VERSION</span> <span style="color:#e6db74">0.1.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">DESCRIPTION</span> <span style="color:#e6db74">&#34;Your project description&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">LANGUAGES</span> <span style="color:#e6db74">CXX</span>
</span></span><span style="display:flex;"><span>)<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># Add OpenMP support
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>find_package(<span style="color:#e6db74">OpenMP</span> <span style="color:#e6db74">REQUIRED</span>)<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#75715e"># Add OpenMP to your targets
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>target_link_libraries(<span style="color:#e6db74">your_target</span> <span style="color:#e6db74">PRIVATE</span> <span style="color:#e6db74">OpenMP::OpenMP_CXX</span>)<span style="color:#960050;background-color:#1e0010">
</span></span></span></code></pre></div><h3 id="4-build-the-project">4. Build the Project</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mkdir build
</span></span><span style="display:flex;"><span>cd build
</span></span><span style="display:flex;"><span>cmake ..
</span></span><span style="display:flex;"><span>cmake --build .
</span></span></code></pre></div><h2 id="understanding-the-fix">Understanding the Fix</h2>
<ol>
<li>
<p><strong>Why this works</strong>:</p>
<ul>
<li>LLVM&rsquo;s version of Clang includes OpenMP support</li>
<li>Setting compiler paths before <code>project()</code> ensures CMake uses the correct compiler</li>
<li>Environment variables ensure system-wide availability of LLVM tools</li>
</ul>
</li>
<li>
<p><strong>Key Commands</strong>:</p>
<ul>
<li><code>cmake ..</code> - Generates build system using parent directory&rsquo;s CMakeLists.txt</li>
<li><code>cmake --build .</code> - Compiles the code using generated build system</li>
</ul>
</li>
<li>
<p><strong>Environment Variables Purpose</strong>:</p>
<ul>
<li><code>PATH</code>: Makes LLVM binaries available system-wide</li>
<li><code>CC/CXX</code>: Sets default C/C++ compilers</li>
<li><code>LDFLAGS/CPPFLAGS</code>: Provides linker and preprocessor with LLVM library locations</li>
</ul>
</li>
</ol>
<h2 id="verification">Verification</h2>
<p>To verify the setup:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>which clang++  <span style="color:#75715e"># Should show /opt/homebrew/opt/llvm/bin/clang++</span>
</span></span><span style="display:flex;"><span>clang++ --version  <span style="color:#75715e"># Should show LLVM version</span>
</span></span></code></pre></div><p>Your OpenMP programs should now compile and run successfully on macOS.</p>

                        </div>
                    </article>
                    <hr>
                    
                </div>
            </section>
        </div>
    </main>
    <footer>
    <p>&copy; 2025 Huy Pham. All rights reserved.</p>
    <p>
        <a href="https://github.com/yourusername" target="_blank">GitHub</a> | 
        <a href="https://linkedin.com/in/yourusername" target="_blank">LinkedIn</a>
    </p>
</footer>
</body>

</html>