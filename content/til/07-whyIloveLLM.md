---
title: "Why I Love Working with Large Language Models"
date: "2025-03-14T13:00:00Z"
draft: false
tags: ["ai", "llm", "productivity", "programming"]
---

Because they are stupid.

Normal people workflows with LLMs are like this, and think they can learn from them:
- I have a problem X
- I ask LLM to solve it
- LLM gives me a solution
- I take the solution and run it
- It works, I am happy
- Then I ask LLM to explain the solution -> I learned it.

It's fine, but in the best case.

Then how I work with LLMs:
- I have a problem X
- I ask LLM to solve it
- LLM gives me a solution
- This goddamn solution is hallucinated, the apis was from the sky (but sometimes I found they come up better name than the code owners)
- I go read the documents, read the code of the LLMs.
- Fix it, it works, I am happy.

Instead of passively ask LLM to explain the solution, I actually just solve the problem by myself just by stepping backward and looking at the problem. 

I think it's better.